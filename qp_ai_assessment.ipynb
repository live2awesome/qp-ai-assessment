{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvcp85JQ4HkyYGRpU530L6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/live2awesome/qp-ai-assessment/blob/main/qp_ai_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "S-rCu9y-0K7P"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# don't required to mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install the necessary package\n",
        "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
        "# %pip install -q arxiv pymupdf faiss-cpu\n"
      ],
      "metadata": {
        "id": "rPFkJujg0meh"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints._common import NVEModel"
      ],
      "metadata": {
        "id": "azODYksD06wJ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Kindly paste the NVIDIA API Key given in readme on github repository**"
      ],
      "metadata": {
        "id": "XkLeup_RECB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)\n",
        "\n",
        "from getpass import getpass\n",
        "import requests\n",
        "import os\n",
        "\n",
        "hard_reset = False\n",
        "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
        "    try:\n",
        "        assert not hard_reset\n",
        "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
        "        assert response.get('nvapi_key')\n",
        "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
        "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
        "    except: pass\n",
        "    hard_reset = False\n",
        "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
        "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
        "\n",
        "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
        "NVEModel().available_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ6lFIVs0_ny",
        "outputId": "9885e106-49bf-4cf2-a301-7ae3e6ef66af"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved NVIDIA_API_KEY beginning with \"nvapi-QpK...\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
              " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
              " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
              " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
              " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
              " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
              " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
              " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
              " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
              " 'playground_sd_video': 'a529a395-a7a0-4708-b4df-eb5e41d5ff60',\n",
              " 'playground_sdxl_turbo': '0ba5e4c7-4540-4a02-b43a-43980067f4af',\n",
              " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
              " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
              " 'playground_seamless': '72ad9555-2e3d-4e73-9050-a37129064743',\n",
              " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
              " 'playground_llama2_code_70b': '2ae529dc-f728-4a46-9b8d-2697213666d8',\n",
              " 'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
              " 'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
              " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f',\n",
              " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
              " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
              " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
              " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92',\n",
              " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520'}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "print(\"Loading Documents\")\n",
        "docs = [\n",
        "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
        "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
        "]\n",
        "\n",
        "## Cutting the paper short if references is included.\n",
        "for doc in docs:\n",
        "    content = doc[0].page_content\n",
        "    if \"References\" in content:\n",
        "        doc[0].page_content = content[:content.index(\"References\")]\n",
        "\n",
        "## Split the documents and also filter out stubs (overly short chunks)\n",
        "print(\"Chunking Documents\")\n",
        "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
        "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
        "\n",
        "## Make some custom Chunks to give big-picture details\n",
        "doc_string = \"Available Documents:\"\n",
        "doc_metadata = []\n",
        "for chunks in docs_chunks:\n",
        "    metadata = getattr(chunks[0], 'metadata', {})\n",
        "    doc_string += \"\\n - \" + metadata.get('Title')\n",
        "    doc_metadata += [str(metadata)]\n",
        "\n",
        "extra_chunks = [doc_string] + doc_metadata\n",
        "\n",
        "## Printing out some summary information for reference\n",
        "pprint(doc_string, '\\n')\n",
        "for i, chunks in enumerate(docs_chunks):\n",
        "    print(f\"Document {i}\")\n",
        "    print(f\" - Metadata: {chunks[0].metadata}\")\n",
        "    print(f\" - # Chunks: {len(chunks)}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "aT-rlUpV1Sky",
        "outputId": "81e2150b-97d8-4731-95d3-8f1388d357b6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Documents\n",
            "Chunking Documents\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0\n",
            " - Metadata: {'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
            " - # Chunks: 34\n",
            "\n",
            "Document 1\n",
            " - Metadata: {'Published': '2019-05-24', 'Title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'Authors': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova', 'Summary': 'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'}\n",
            " - # Chunks: 42\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from faiss import IndexFlatL2\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=None)\n",
        "\n",
        "## Construct series of document vector stores\n",
        "print(\"Constructing Vector Stores\")\n",
        "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
        "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mrqp0HB1cVc",
        "outputId": "b67c797c-029f-4d46-d41d-8f4981407a45"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing Vector Stores\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dims = len(embedder.embed_query(\"test\"))\n",
        "def default_FAISS():\n",
        "    '''Useful utility for making an empty FAISS vectorstore'''\n",
        "    return FAISS(\n",
        "        embedding_function=embedder,\n",
        "        index=IndexFlatL2(embed_dims),\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={},\n",
        "        normalize_L2=False\n",
        "    )\n",
        "\n",
        "def aggregate_vstores(vectorstores):\n",
        "    ## Initialize an empty FAISS Index and merge others into it\n",
        "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
        "    agg_vstore = default_FAISS()\n",
        "    for vstore in vectorstores:\n",
        "        agg_vstore.merge_from(vstore)\n",
        "    return agg_vstore\n",
        "\n",
        "if 'docstore' not in globals():\n",
        "    ## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
        "    docstore = aggregate_vstores(vecstores)\n",
        "\n",
        "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGSRKK1o1k_i",
        "outputId": "b6bb9b7a-1ee3-4efe-b8eb-e3d25381278b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructed aggregate docstore with 223 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorstore is constructed and chuking is done and embedder is also avaliable"
      ],
      "metadata": {
        "id": "MMf0RJGV7iQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Utility Runnables/Methods\n",
        "def RPrint(preface=\"\"):\n",
        "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
        "    def print_and_return(x, preface):\n",
        "        print(f\"{preface}{x}\")\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ],
      "metadata": {
        "id": "NmXOGdai20ZP"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name:\n",
        "            out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str"
      ],
      "metadata": {
        "id": "YIyCMkyP2vip"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import gradio as gr\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
        "llm = ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser()\n",
        "convstore = default_FAISS()\n",
        "\n",
        "\n",
        "initial_msg = (\n",
        "    \"Hello! I am a document chat agent here to help the user!\"\n",
        "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked: {input}\\n\\n\"\n",
        "    \" From this, we have retrieved the following potentially-useful info: \"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used.If you don't know the answer, just say you don't know. Don't try to make up an answer.)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "\n",
        "retrieval_chain = (\n",
        "    {'context' : docstore.as_retriever() | long_reorder | docs2str ,\n",
        "     'input' : (lambda x: x)}\n",
        "    |RPrint()\n",
        "    | RunnableAssign({'output' : chat_prompt | llm})\n",
        ")\n",
        "\n",
        "\n",
        "stream_chain = chat_prompt | llm\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "    ## First perform the retrieval based on the input message\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "    line_buffer = \"\"\n",
        "\n",
        "    ## Then, stream the results of the stream_chain\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "        if not return_buffer:\n",
        "            line_buffer += token\n",
        "            if \"\\n\" in line_buffer:\n",
        "                line_buffer = \"\"\n",
        "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
        "                line_buffer = \"\"\n",
        "                yield \"\\n\"\n",
        "                token = \"  \" + token.lstrip()\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xCQxAFxh1uUa"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "## Start of Agent Event Loop\n",
        "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
        "\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ4keNYcEwAd",
        "outputId": "d4fe5938-64c3-41ff-98b6-37201b2a61ae"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'context': '[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] 80%, on par with the level of agreement among human experts, establishing a foundation for an\\nLLM-based evaluation framework.\\nAcknowledgement\\nThis project is partly supported by gifts from Anyscale, Astronomer, Google, IBM, Intel, Lacework,\\nMicrosoft, MBZUAI, Samsung SDS, Uber, and VMware. Lianmin Zheng is supported by a Meta\\nPh.D. Fellowship. We extend our thanks to Xinyang Geng, Hao Liu, Eric Wallace, Xuecheng Li,\\nTianyi Zhang, Qirong Ho, and Kevin Lin for their insightful discussions.\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with speciﬁc information required to\\ngenerate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see §4.5).\\n4.3\\nJeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack-\\naugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.\\nOur results highlight the beneﬁts of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] the news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\n', 'input': 'Tell me about RAG!'}\n",
            "RAG, or Retrieval-Augmented Generation, is a method used in natural language\n",
            "  processing (NLP) tasks that involves the use of both parametric and non-parametric memory\n",
            "  components that are pre-trained and pre-loaded with extensive knowledge. This approach allows\n",
            "  for the ability to access knowledge without additional training.\n",
            "\n",
            "RAG models have been shown to achieve state-of-the-art results on open Natural\n",
            "  Questions, WebQuestions, and CuratedTrec, as well as strongly outperform recent approaches\n",
            "  that use specialized pre-training objectives on TriviaQA. Despite these being extractive\n",
            "  tasks, unconstrained generation has been found to outperform previous extractive\n",
            "  approaches.\n",
            "\n",
            "In the context of knowledge-intensive NLP tasks, RAG models approach state-of-the-art\n",
            "  model performance and generate factually correct text more often than other models, such\n",
            "  as BART. RAG models have also been shown to hallucinate less and generate more diverse\n",
            "  text than BART.\n",
            "\n",
            "(Sources: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, Judging\n",
            "  LLM-as-a-Judge with MT-Bench and Chatbot Arena)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
        "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
        "\n",
        "try:\n",
        "    demo.launch(debug=True, share=True, show_api=False)\n",
        "    demo.close()\n",
        "except Exception as e:\n",
        "    demo.close()\n",
        "    print(e)\n",
        "    raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "S953VTeR1oAi",
        "outputId": "350a1d05-3c48-4e1b-a733-615e22875c61"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://a41c1dffb4ec181ecc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a41c1dffb4ec181ecc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a41c1dffb4ec181ecc.gradio.live\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    }
  ]
}